{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13583714,"sourceType":"datasetVersion","datasetId":8630000}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===========================\n# Temporal Transformer - Train & Save Best Model + CSV Embeddings\n# Kaggle-ready (uses GPU if available)\n# ===========================\nimport os, math, random, time, copy\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# sklearn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n\n# -----------------------\n# Reproducibility\n# -----------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# -----------------------\n# SETTINGS / HYPERPARAMS\n# -----------------------\nCSV_PATH = \"/kaggle/input/transformer-training-dataset/temporal_vitals_6h_labeled.csv\"\nOUT_MODEL_PATH = \"/kaggle/working/best_temporal_transformer.pt\"\nOUT_CSV_PATH = \"/kaggle/working/temporal_embeddings_predictions.csv\"\nOUT_SCALER_PATH = \"/kaggle/working/scaler.npy\"\nOUT_IMPUTER_PATH = \"/kaggle/working/imputer.npy\"\n\nID_COL = \"subject_id\"\nSTAY_COL = \"stay_id\"\nTIME_COL = \"hour_bin\"\nLABEL_COL = \"ventilation_within_12h\"\n\nBATCH_SIZE = 64\nEMBED_DIM = 128\nNHEAD = 4\nNUM_LAYERS = 3\nDROPOUT = 0.2\nEPOCHS = 100\nPATIENCE = 8\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nCLIP_NORM = 1.0\nNUM_WORKERS = 2\nMAX_SEQ_LEN_LIMIT = None  # None -> use dataset max\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n\n# -----------------------\n# 1) Load data\n# -----------------------\ndf = pd.read_csv(CSV_PATH)\nprint(\"Raw shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\n\n# Ensure required cols exist\nfor c in (ID_COL, STAY_COL, TIME_COL, LABEL_COL):\n    assert c in df.columns, f\"Missing required column: {c}\"\n\n# -----------------------\n# 2) Preprocess features\n# -----------------------\n# exclude ids/time/label from features\nexclude = {ID_COL, \"hadm_id\", STAY_COL, TIME_COL, LABEL_COL}\nfeature_cols = [c for c in df.columns if c not in exclude and df[c].dtype != object]\nprint(f\"Using {len(feature_cols)} feature columns.\")\n\n# sort by stay + time\ndf = df.sort_values([STAY_COL, TIME_COL]).reset_index(drop=True)\n\n# Per-stay forward/back-fill then global mean imputing\nfor col in feature_cols:\n    # transform keeps index alignment\n    df[col] = df.groupby(STAY_COL)[col].transform(lambda s: s.ffill().bfill())\n\n# global mean imputer for any remaining NaNs (e.g., a whole stay had NaN)\nimputer = SimpleImputer(strategy=\"mean\")\ndf[feature_cols] = imputer.fit_transform(df[feature_cols])\n\n# Standard scaling (fit on full dataset here; in strict evaluation use train-only)\nscaler = StandardScaler()\ndf[feature_cols] = scaler.fit_transform(df[feature_cols])\n\n# Save scaler & imputer (numpy)\nnp.save(OUT_SCALER_PATH, np.array([scaler.mean_, scaler.scale_], dtype=object), allow_pickle=True)\nnp.save(OUT_IMPUTER_PATH, np.array([imputer.statistics_], dtype=object), allow_pickle=True)\n\n# Determine max seq length\nseq_counts = df.groupby(STAY_COL)[TIME_COL].nunique().values\nmax_seq_len = int(seq_counts.max()) if MAX_SEQ_LEN_LIMIT is None else min(MAX_SEQ_LEN_LIMIT, int(seq_counts.max()))\nprint(\"Max seq length used:\", max_seq_len)\n\n# -----------------------\n# 3) Train-Val split by stay_id (no leakage)\n# -----------------------\nstays = df[STAY_COL].unique()\ntrain_stays, val_stays = train_test_split(stays, test_size=0.2, random_state=SEED, shuffle=True)\ntrain_df = df[df[STAY_COL].isin(train_stays)].reset_index(drop=True)\nval_df = df[df[STAY_COL].isin(val_stays)].reset_index(drop=True)\nprint(f\"Train stays: {len(train_stays)}, Val stays: {len(val_stays)}\")\n\n# -----------------------\n# 4) Dataset + collate function (padding + mask)\n# -----------------------\nclass TemporalStayDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, stay_col: str, id_col: str, time_col: str, feature_cols: List[str], label_col: str, max_len: int):\n        self.feature_cols = feature_cols\n        self.label_col = label_col\n        self.max_len = max_len\n        # group stays\n        self.rows = []\n        for stay_id, g in df.groupby(stay_col):\n            g = g.sort_values(time_col)\n            feats = g[feature_cols].to_numpy(dtype=np.float32)\n            label = int(g[label_col].iloc[0])\n            subject_id = g[id_col].iloc[0]\n            self.rows.append((subject_id, stay_id, feats, label))\n    def __len__(self):\n        return len(self.rows)\n    def __getitem__(self, idx):\n        return self.rows[idx]\n\ndef collate_fn(batch):\n    # batch: list of (subject_id, stay_id, feats (T,F), label)\n    batch_size = len(batch)\n    n_feat = batch[0][2].shape[1]\n    seq_lens = [min(x[2].shape[0], max_seq_len) for x in batch]\n    T = max(seq_lens)\n    X = np.zeros((batch_size, T, n_feat), dtype=np.float32)\n    pad_mask = np.ones((batch_size, T), dtype=bool)  # True indicates padding for transformer.mask\n    labels = np.zeros((batch_size,), dtype=np.float32)\n    subject_ids = []\n    stay_ids = []\n    for i, (subject_id, stay_id, feats, label) in enumerate(batch):\n        L = min(feats.shape[0], T)\n        X[i, :L, :] = feats[:L]\n        pad_mask[i, :L] = False\n        labels[i] = label\n        subject_ids.append(subject_id)\n        stay_ids.append(stay_id)\n    return {\n        \"X\": torch.from_numpy(X),             # (B, T, F)\n        \"pad_mask\": torch.from_numpy(pad_mask), # (B, T) bool\n        \"y\": torch.from_numpy(labels).unsqueeze(1), # (B,1)\n        \"subject_ids\": subject_ids,\n        \"stay_ids\": stay_ids\n    }\n\ntrain_dataset = TemporalStayDataset(train_df, STAY_COL, ID_COL, TIME_COL, feature_cols, LABEL_COL, max_seq_len)\nval_dataset = TemporalStayDataset(val_df, STAY_COL, ID_COL, TIME_COL, feature_cols, LABEL_COL, max_seq_len)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n\nprint(\"Train samples:\", len(train_dataset), \"Val samples:\", len(val_dataset))\n\n# -----------------------\n# 5) Model (Transformer Encoder + classification head)\n# -----------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model:int, max_len:int=1024):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    def forward(self, x: torch.Tensor):\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len, :].to(x.device)\n\nclass TemporalTransformerClassifier(nn.Module):\n    def __init__(self, in_dim:int, emb_dim:int=EMBED_DIM, nhead:int=NHEAD, nlayers:int=NUM_LAYERS, dropout:float=DROPOUT):\n        super().__init__()\n        self.input_fc = nn.Linear(in_dim, emb_dim)\n        self.pos = PositionalEncoding(emb_dim, max_len=max_seq_len)\n        enc_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=emb_dim*4, dropout=dropout, batch_first=True)\n        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(emb_dim, 1)\n    def forward(self, x: torch.Tensor, src_key_padding_mask: torch.Tensor):\n        # x: (B, T, F)\n        x = self.input_fc(x)              # (B, T, D)\n        x = self.pos(x)                   # add positional\n        # src_key_padding_mask: (B, T) True for padding positions\n        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # (B, T, D)\n        # mean pool over valid steps\n        valid_mask = (~src_key_padding_mask).unsqueeze(-1).to(x.device)  # (B, T, 1)\n        sum_hidden = (x * valid_mask).sum(dim=1)                         # (B, D)\n        lengths = valid_mask.sum(dim=1).clamp(min=1).to(x.device)        # (B,1)\n        pooled = sum_hidden / lengths                                    # (B, D)\n        pooled = self.dropout(pooled)\n        logits = self.classifier(pooled)                                 # (B,1)\n        return logits, pooled\n\nmodel = TemporalTransformerClassifier(len(feature_cols), EMBED_DIM, NHEAD, NUM_LAYERS, DROPOUT).to(DEVICE)\n\n# -----------------------\n# 6) Loss, optimizer, scheduler\n# Use class weighting to handle imbalance\n# -----------------------\n# compute pos weight for BCEWithLogitsLoss\nall_labels = train_df[LABEL_COL].astype(int).values\npos_count = all_labels.sum()\nneg_count = len(all_labels) - pos_count\npos_weight = torch.tensor([(neg_count / (pos_count + 1e-9))]).to(DEVICE)  # avoid zero divide\nprint(\"Pos weight:\", float(pos_weight))\n\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n\n# -----------------------\n# 7) Training / Evaluation functions\n# -----------------------\ndef evaluate_model(loader):\n    model.eval()\n    losses, preds_list, labels_list = [], [], []\n    with torch.no_grad():\n        for batch in loader:\n            X = batch[\"X\"].to(DEVICE)\n            pad_mask = batch[\"pad_mask\"].to(DEVICE)\n            y = batch[\"y\"].to(DEVICE)\n            logits, _ = model(X, src_key_padding_mask=pad_mask)\n            loss = criterion(logits, y)\n            losses.append(loss.item() * y.size(0))\n            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n            preds_list.extend(probs.tolist())\n            labels_list.extend(y.cpu().numpy().flatten().tolist())\n    if len(labels_list) == 0:\n        return float(\"nan\"), {}\n    loss_epoch = sum(losses) / len(loader.dataset)\n    try:\n        auc = roc_auc_score(labels_list, preds_list)\n    except Exception:\n        auc = float(\"nan\")\n    aupr = average_precision_score(labels_list, preds_list) if len(set(labels_list))>1 else float(\"nan\")\n    preds_binary = [1 if p >= 0.5 else 0 for p in preds_list]\n    acc = accuracy_score(labels_list, preds_binary)\n    f1 = f1_score(labels_list, preds_binary, zero_division=0)\n    metrics = {\"loss\": loss_epoch, \"auc\": auc, \"aupr\": aupr, \"acc\": acc, \"f1\": f1}\n    return loss_epoch, metrics\n\n# -----------------------\n# 8) Training loop with early stopping\n# -----------------------\nbest_val_loss = float(\"inf\")\nbest_state = None\npatience = 0\nhistory = []\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    epoch_loss = 0.0\n    n_samples = 0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n    for batch in pbar:\n        X = batch[\"X\"].to(DEVICE)\n        pad_mask = batch[\"pad_mask\"].to(DEVICE)\n        y = batch[\"y\"].to(DEVICE)\n        optimizer.zero_grad()\n        logits, _ = model(X, src_key_padding_mask=pad_mask)\n        loss = criterion(logits, y)\n        loss.backward()\n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n        optimizer.step()\n        epoch_loss += loss.item() * y.size(0)\n        n_samples += y.size(0)\n    train_loss = epoch_loss / n_samples if n_samples>0 else float(\"nan\")\n    val_loss, val_metrics = evaluate_model(val_loader)\n    scheduler.step(val_loss)\n    history.append({\"epoch\": epoch, \"train_loss\": train_loss, **{f\"val_{k}\":v for k,v in val_metrics.items()}})\n    print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val AUROC: {val_metrics['auc']:.4f} | Val AUPR: {val_metrics['aupr']:.4f} | Val Acc: {val_metrics['acc']:.4f} | Val F1: {val_metrics['f1']:.4f}\")\n    # save best\n    if val_metrics[\"loss\"] < best_val_loss - 1e-6:\n        best_val_loss = val_metrics[\"loss\"]\n        best_state = copy.deepcopy(model.state_dict())\n        torch.save(best_state, OUT_MODEL_PATH)\n        patience = 0\n        print(\"  -> New best model saved.\")\n    else:\n        patience += 1\n        if patience >= PATIENCE:\n            print(\"Early stopping triggered. Patience exceeded.\")\n            break\n\n# -----------------------\n# 9) Load best model and create embeddings + predictions for ALL stays (train+val or entire dataset)\n# -----------------------\nif best_state is None:\n    best_state = model.state_dict()\nmodel.load_state_dict(best_state)\nmodel.eval()\n\n# create a DataLoader for entire dataset (train+val) to produce embeddings; use df grouped by stay\nall_dataset = TemporalStayDataset(df, STAY_COL, ID_COL, TIME_COL, feature_cols, LABEL_COL, max_seq_len)\nall_loader = DataLoader(all_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n\nsubject_list, stay_list, label_list, prob_list, emb_list = [], [], [], [], []\nwith torch.no_grad():\n    for batch in tqdm(all_loader, desc=\"Generating embeddings for all stays\"):\n        X = batch[\"X\"].to(DEVICE)\n        pad_mask = batch[\"pad_mask\"].to(DEVICE)\n        logits, emb = model(X, src_key_padding_mask=pad_mask)\n        probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n        emb_np = emb.cpu().numpy()\n        subject_list.extend(batch[\"subject_ids\"])\n        stay_list.extend(batch[\"stay_ids\"])\n        # labels: get from df (first row for stay)\n        for sid in batch[\"stay_ids\"]:\n            lab = int(df[df[STAY_COL] == sid][LABEL_COL].iloc[0])\n            label_list.append(lab)\n        prob_list.extend(probs.tolist())\n        emb_list.extend(emb_np.tolist())\n\n# Build output DataFrame\nemb_dim = len(emb_list[0]) if len(emb_list)>0 else EMBED_DIM\nemb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\nout_df = pd.DataFrame(emb_list, columns=emb_cols)\nout_df.insert(0, \"stay_id\", stay_list)\nout_df.insert(0, \"subject_id\", subject_list)\nout_df[\"true_label\"] = label_list\nout_df[\"pred_prob\"] = prob_list\n\nout_df.to_csv(OUT_CSV_PATH, index=False)\nprint(\"Saved embeddings & predictions to:\", OUT_CSV_PATH)\nprint(\"Output shape:\", out_df.shape)\n\n# -----------------------\n# 10) Save training history\n# -----------------------\nhist_df = pd.DataFrame(history)\nhist_df.to_csv(\"/kaggle/working/training_history.csv\", index=False)\nprint(\"Training history saved to /kaggle/working/training_history.csv\")\nprint(\"Best model path:\", OUT_MODEL_PATH)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:44:03.910659Z","iopub.execute_input":"2025-11-02T07:44:03.910944Z","iopub.status.idle":"2025-11-02T07:47:23.995101Z","shell.execute_reply.started":"2025-11-02T07:44:03.910924Z","shell.execute_reply":"2025-11-02T07:47:23.994124Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nRaw shape: (43080, 23)\nColumns: ['subject_id', 'hadm_id', 'stay_id', 'hour_bin', 'mean_value_dbp', 'mean_value_fio2', 'mean_value_gcs_motor', 'mean_value_gcs_total', 'mean_value_heart_rate', 'mean_value_resp_rate', 'mean_value_sbp', 'mean_value_spo2', 'mean_value_temperature', 'std_value_dbp', 'std_value_fio2', 'std_value_gcs_motor', 'std_value_gcs_total', 'std_value_heart_rate', 'std_value_resp_rate', 'std_value_sbp', 'std_value_spo2', 'std_value_temperature', 'ventilation_within_12h']\nUsing 18 feature columns.\nMax seq length used: 6\nTrain stays: 5744, Val stays: 1436\nTrain samples: 5744 Val samples: 1436\nPos weight: 6.450064850841666\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 001 | Train Loss: 0.7895 | Val Loss: 0.9075 | Val AUROC: 0.8412 | Val AUPR: 0.6898 | Val Acc: 0.9060 | Val F1: 0.6650\n  -> New best model saved.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 002 | Train Loss: 0.7193 | Val Loss: 0.8659 | Val AUROC: 0.8450 | Val AUPR: 0.7120 | Val Acc: 0.9269 | Val F1: 0.7273\n  -> New best model saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 003 | Train Loss: 0.6580 | Val Loss: 0.7985 | Val AUROC: 0.8508 | Val AUPR: 0.7117 | Val Acc: 0.9241 | Val F1: 0.7241\n  -> New best model saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 004 | Train Loss: 0.6400 | Val Loss: 0.9905 | Val AUROC: 0.8524 | Val AUPR: 0.7086 | Val Acc: 0.9178 | Val F1: 0.7050\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 005 | Train Loss: 0.6440 | Val Loss: 0.8530 | Val AUROC: 0.8528 | Val AUPR: 0.7005 | Val Acc: 0.9262 | Val F1: 0.7022\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 006 | Train Loss: 0.6344 | Val Loss: 0.8363 | Val AUROC: 0.8512 | Val AUPR: 0.7393 | Val Acc: 0.9318 | Val F1: 0.7407\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 007 | Train Loss: 0.6417 | Val Loss: 0.7979 | Val AUROC: 0.8577 | Val AUPR: 0.7273 | Val Acc: 0.9123 | Val F1: 0.6912\n  -> New best model saved.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 008 | Train Loss: 0.6368 | Val Loss: 0.9394 | Val AUROC: 0.8470 | Val AUPR: 0.7144 | Val Acc: 0.9102 | Val F1: 0.6892\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 009 | Train Loss: 0.6219 | Val Loss: 0.9058 | Val AUROC: 0.8403 | Val AUPR: 0.6859 | Val Acc: 0.9032 | Val F1: 0.6651\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 010 | Train Loss: 0.6179 | Val Loss: 0.9194 | Val AUROC: 0.8507 | Val AUPR: 0.7022 | Val Acc: 0.9345 | Val F1: 0.7500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 011 | Train Loss: 0.5975 | Val Loss: 0.8598 | Val AUROC: 0.8431 | Val AUPR: 0.6988 | Val Acc: 0.9185 | Val F1: 0.7068\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 012 | Train Loss: 0.5882 | Val Loss: 0.8794 | Val AUROC: 0.8286 | Val AUPR: 0.7349 | Val Acc: 0.9046 | Val F1: 0.6821\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 013 | Train Loss: 0.5759 | Val Loss: 0.8315 | Val AUROC: 0.8453 | Val AUPR: 0.7443 | Val Acc: 0.8726 | Val F1: 0.6273\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 014 | Train Loss: 0.5834 | Val Loss: 0.8382 | Val AUROC: 0.8625 | Val AUPR: 0.7617 | Val Acc: 0.9220 | Val F1: 0.7172\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/100:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 015 | Train Loss: 0.5668 | Val Loss: 0.8421 | Val AUROC: 0.8603 | Val AUPR: 0.7217 | Val Acc: 0.9032 | Val F1: 0.6790\nEarly stopping triggered. Patience exceeded.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating embeddings for all stays:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb6d870912364423a06dd81cb6371907"}},"metadata":{}},{"name":"stdout","text":"Saved embeddings & predictions to: /kaggle/working/temporal_embeddings_predictions.csv\nOutput shape: (7180, 132)\nTraining history saved to /kaggle/working/training_history.csv\nBest model path: /kaggle/working/best_temporal_transformer.pt\n","output_type":"stream"}],"execution_count":5}]}