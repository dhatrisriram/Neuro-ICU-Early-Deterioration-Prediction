{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13383139,"sourceType":"datasetVersion","datasetId":8491501},{"sourceId":13473508,"sourceType":"datasetVersion","datasetId":8553189},{"sourceId":13583767,"sourceType":"datasetVersion","datasetId":8630042},{"sourceId":13584189,"sourceType":"datasetVersion","datasetId":8630352}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Paths\nICU_PATH = \"/kaggle/input/neuro-icu/icu/icu/\"\nDNN_PATH = \"/kaggle/input/dnn-part/\"\n\n# --- Load base files ---\nicu = pd.read_csv(\n    ICU_PATH + \"icustays.csv\",\n    usecols=[\"stay_id\", \"hadm_id\", \"subject_id\", \"first_careunit\", \"los\"]\n)\npatients = pd.read_csv(\n    DNN_PATH + \"patients.csv\",\n    usecols=[\"subject_id\", \"gender\", \"anchor_age\"]\n)\nadmissions = pd.read_csv(\n    DNN_PATH + \"admissions.csv\",\n    usecols=[\"hadm_id\", \"admittime\", \"dischtime\"]\n)\n\n# --- Merge static demographic info ---\nstatic_df = (\n    icu.merge(patients, on=\"subject_id\", how=\"left\")\n       .merge(admissions, on=\"hadm_id\", how=\"left\")\n)\n\n# --- Load your engineered features + label ---\nlabels = pd.read_csv(\"/kaggle/input/static-features-dataset/model_neuro6h_scaled_full (1).csv\")\nstatic_df = static_df.merge(\n    labels[[\"subject_id\", \"hadm_id\", \"ventilation_within_12h\"]],\n    on=[\"subject_id\", \"hadm_id\"],\n    how=\"left\",\n    suffixes=(\"\", \"_dup\")\n)\n\n# --- Clean duplicate columns ---\nfor col in static_df.columns:\n    if col.endswith(\"_dup\"):\n        base = col[:-4]\n        if base in static_df.columns:\n            static_df[base] = static_df[base].combine_first(static_df[col])\n        static_df.drop(columns=[col], inplace=True)\n\n# --- Rename columns ---\nstatic_df.rename(columns={\n    \"anchor_age\": \"age\",\n    \"ventilation_within_12h\": \"ventilated_12h\"\n}, inplace=True)\n\n# --- Keep only the final DNN input columns ---\nfinal_static = static_df[\n    static_df[\"ventilated_12h\"].notna()\n][[\n    \"subject_id\",\n    \"stay_id\",\n    \"hadm_id\",\n    \"age\",\n    \"gender\",\n    \"los\",\n    \"first_careunit\",\n    \"ventilated_12h\"\n]]\n\n# Convert label to int (0/1)\nfinal_static[\"ventilated_12h\"] = final_static[\"ventilated_12h\"].astype(int)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:31:58.867088Z","iopub.execute_input":"2025-11-13T06:31:58.867650Z","iopub.status.idle":"2025-11-13T06:32:02.117481Z","shell.execute_reply.started":"2025-11-13T06:31:58.867624Z","shell.execute_reply":"2025-11-13T06:32:02.116862Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Load engineered static dataset (scaled features + label)\nstatic_df = pd.read_csv(\"/kaggle/input/static-features-dataset/model_neuro6h_scaled_full (1).csv\")\ndnn_old=pd.read_csv(\"/kaggle/input/dnn-old-file/static_features_labeled (1).csv\")\n\n# Load demographics from your neuro ICU master table\n# Make sure neuro_icu is loaded before this step!\ndemo_df = dnn_old[['stay_id','subject_id','hadm_id','age','gender','first_careunit','los']].drop_duplicates()\n\n# Merge on IDs\nfinal_static = static_df.merge(\n    demo_df,\n    on=[\"subject_id\",\"hadm_id\"],\n    how=\"left\"\n)\n\n# Keep label at end\nlabel = final_static['ventilation_within_12h']\nfinal_static = final_static.drop(columns=['ventilation_within_12h'])\nfinal_static['ventilation_within_12h'] = label.astype(int)\n\nprint(\"Shape:\", final_static.shape)\nprint(final_static['ventilation_within_12h'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:32:04.395177Z","iopub.execute_input":"2025-11-13T06:32:04.395443Z","iopub.status.idle":"2025-11-13T06:32:04.454893Z","shell.execute_reply.started":"2025-11-13T06:32:04.395421Z","shell.execute_reply":"2025-11-13T06:32:04.453995Z"}},"outputs":[{"name":"stdout","text":"Shape: (8274, 25)\nventilation_within_12h\n0    7085\n1    1189\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Combine LOS columns (use los_x first, if missing use los_y)\nfinal_static[\"los\"] = final_static[\"los_x\"].fillna(final_static[\"los_y\"])\n\n# Drop old LOS columns\nfinal_static.drop(columns=[\"los_x\", \"los_y\"], inplace=True)\n\n# Desired first columns\nfront_cols = [\n    \"subject_id\",\n    \"stay_id\",\n    \"hadm_id\",\n    \"age\",\n    \"gender\",\n    \"first_careunit\",\n    \"los\",\n]\n\n# Move these to front, keep rest in original order\nremaining_cols = [c for c in final_static.columns if c not in front_cols]\n\nfinal_static = final_static[front_cols + remaining_cols]\n\n# Save\nfinal_static.to_csv(\"static_features_final.csv\", index=False)\n\nprint(\"Shape:\", final_static.shape)\nprint(final_static['ventilation_within_12h'].value_counts())\n\nprint(final_static.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:32:08.047722Z","iopub.execute_input":"2025-11-13T06:32:08.048442Z","iopub.status.idle":"2025-11-13T06:32:08.251206Z","shell.execute_reply.started":"2025-11-13T06:32:08.048408Z","shell.execute_reply":"2025-11-13T06:32:08.250395Z"}},"outputs":[{"name":"stdout","text":"Shape: (8274, 24)\nventilation_within_12h\n0    7085\n1    1189\nName: count, dtype: int64\n   subject_id   stay_id     hadm_id  age gender      first_careunit  \\\n0  10002348.0  32610785  22725460.0   77      F  Neuro Intermediate   \n1  10004113.0  39774222  29879900.0   33      M      Neuro Stepdown   \n2  10010399.0  38121508  24825793.0   54      F  Neuro Intermediate   \n3  10011507.0  31950308  24996125.0   41      M  Neuro Intermediate   \n4  10011938.0  36750867  28265343.0   57      F  Neuro Intermediate   \n\n         los  vital_mean  vital_std  vital_min  ...  Ischemic_Stroke  \\\n0   9.792512   -0.088098  -0.043532   0.225534  ...              1.0   \n1   1.060683   -0.027577  -0.042322   0.225534  ...              0.0   \n2   2.294120   -0.070269  -0.029934   0.225534  ...              0.0   \n3  11.761308   -0.028623  -0.005287  -0.277051  ...              0.0   \n4  20.993843   -0.086247  -0.036761  -0.025759  ...              0.0   \n\n   Hemorrhage  Aneurysm  Seizure  Brain_Tumor  Hydrocephalus  AVM  \\\n0         0.0       1.0      1.0          1.0            1.0  0.0   \n1         1.0       0.0      1.0          1.0            0.0  1.0   \n2         0.0       0.0      0.0          0.0            0.0  0.0   \n3         0.0       0.0      0.0          0.0            0.0  0.0   \n4         0.0       0.0      0.0          0.0            0.0  0.0   \n\n   Traumatic_Brain_Injury  Spinal_Cord  ventilation_within_12h  \n0                     0.0          0.0                       0  \n1                     0.0          0.0                       0  \n2                     0.0          0.0                       0  \n3                     0.0          0.0                       1  \n4                     0.0          0.0                       0  \n\n[5 rows x 24 columns]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, classification_report\n\n# --------- Load Data ---------\ndf = pd.read_csv(\"/kaggle/working/static_features_final.csv\")\nprint(\"Loaded static dataset:\", df.shape)\n\n# --------- Preprocess ---------\ncat_cols = [\"gender\", \"first_careunit\"]\nnum_cols = [\n    \"age\", \"los\",\n    \"vital_mean\", \"vital_std\", \"vital_min\", \"vital_max\",\n    \"vital_count\", \"total_input_ml\", \"total_output_ml\"\n] + [\n    \"Ischemic_Stroke\",\"Hemorrhage\",\"Aneurysm\",\"Seizure\",\"Brain_Tumor\",\n    \"Hydrocephalus\",\"AVM\",\"Traumatic_Brain_Injury\",\"Spinal_Cord\"\n]\n\ndf = df.copy()\ndf[cat_cols] = df[cat_cols].fillna(\"Unknown\")\ndf[num_cols] = df[num_cols].fillna(df[num_cols].median())\n\nencoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\nscaler = StandardScaler()\n\nX_cat = encoder.fit_transform(df[cat_cols])\nX_num = scaler.fit_transform(df[num_cols])\nX = np.hstack([X_num, X_cat])\ny = df[\"ventilation_within_12h\"].values\n\n# Split AFTER preprocessing\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_t = torch.tensor(X_train, dtype=torch.float32)\ny_train_t = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\nX_val_t = torch.tensor(X_val, dtype=torch.float32)\ny_val_t = torch.tensor(y_val.reshape(-1, 1), dtype=torch.float32)\n\n# --------- Model Definition ---------\nclass DNN(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n        self.classifier = nn.Linear(32, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        logits = self.classifier(features)\n        return logits, features\n\nmodel = DNN(X_train.shape[1])\nuse_focal = False\n\n# --------- Compute pos_weight (Now y_train exists) ---------\nneg, pos = (y_train == 0).sum(), (y_train == 1).sum()\npos_weight = torch.tensor([neg / pos], dtype=torch.float32)\nprint(f\"pos_weight = {pos_weight.item():.2f}\")\n\n# --------- Focal Loss Function ---------\ndef focal_loss(logits, targets, alpha=0.75, gamma=2):\n    prob = torch.sigmoid(logits)\n    bce = nn.functional.binary_cross_entropy_with_logits(\n        logits, targets, reduction='none'\n    )\n    fl = alpha * (1 - prob)**gamma * bce\n    return fl.mean()\n\n# --------- Criterion, Optimizer, Scheduler ---------\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n)\n\n# --------- Training Loop ---------\nbest_val_loss = np.inf\npatience, patience_counter = 10, 0\nepochs = 200\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n\n    logits, _ = model(X_train_t)\n    loss = focal_loss(logits, y_train_t) if use_focal else criterion(logits, y_train_t)\n\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        val_logits, _ = model(X_val_t)\n        val_loss = criterion(val_logits, y_val_t)\n        scheduler.step(val_loss)\n\n    print(f\"Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_state = model.state_dict()\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"â³ Early stopping triggered\")\n            break\n\nmodel.load_state_dict(best_state)\n\n# --------- Threshold Tuning ---------\nmodel.eval()\nwith torch.no_grad():\n    probs = torch.sigmoid(model(X_val_t)[0]).numpy().flatten()\n\nprec, rec, thresholds = precision_recall_curve(y_val, probs)\nf1_scores = 2 * (prec * rec) / (prec + rec + 1e-8)\nbest_idx = np.argmax(f1_scores)\nbest_thresh = thresholds[best_idx]\n\nprint(f\"\\nðŸŽ¯ Best Threshold = {best_thresh:.3f}\")\nprint(f\"Precision={prec[best_idx]:.3f}, Recall={rec[best_idx]:.3f}, F1={f1_scores[best_idx]:.3f}\")\n\nfinal_preds = (probs > best_thresh).astype(int)\n\nprint(\"ROC-AUC:\", roc_auc_score(y_val, probs))\nprint(classification_report(y_val, final_preds))\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Loaded static dataset: (8274, 24)\npos_weight = 6.08\nEpoch 001 | Loss: 1.5998 | Val Loss: 1.6017\nEpoch 002 | Loss: 1.5453 | Val Loss: 1.5548\nEpoch 003 | Loss: 1.5007 | Val Loss: 1.5109\nEpoch 004 | Loss: 1.4778 | Val Loss: 1.4702\nEpoch 005 | Loss: 1.4234 | Val Loss: 1.4328\nEpoch 006 | Loss: 1.3856 | Val Loss: 1.3981\nEpoch 007 | Loss: 1.3571 | Val Loss: 1.3658\nEpoch 008 | Loss: 1.3304 | Val Loss: 1.3356\nEpoch 009 | Loss: 1.2986 | Val Loss: 1.3077\nEpoch 010 | Loss: 1.2835 | Val Loss: 1.2817\nEpoch 011 | Loss: 1.2399 | Val Loss: 1.2577\nEpoch 012 | Loss: 1.2291 | Val Loss: 1.2354\nEpoch 013 | Loss: 1.2172 | Val Loss: 1.2146\nEpoch 014 | Loss: 1.1938 | Val Loss: 1.1956\nEpoch 015 | Loss: 1.1690 | Val Loss: 1.1781\nEpoch 016 | Loss: 1.1602 | Val Loss: 1.1618\nEpoch 017 | Loss: 1.1360 | Val Loss: 1.1468\nEpoch 018 | Loss: 1.1247 | Val Loss: 1.1328\nEpoch 019 | Loss: 1.1075 | Val Loss: 1.1195\nEpoch 020 | Loss: 1.0942 | Val Loss: 1.1069\nEpoch 021 | Loss: 1.0909 | Val Loss: 1.0948\nEpoch 022 | Loss: 1.0796 | Val Loss: 1.0833\nEpoch 023 | Loss: 1.0659 | Val Loss: 1.0721\nEpoch 024 | Loss: 1.0581 | Val Loss: 1.0614\nEpoch 025 | Loss: 1.0493 | Val Loss: 1.0510\nEpoch 026 | Loss: 1.0423 | Val Loss: 1.0409\nEpoch 027 | Loss: 1.0297 | Val Loss: 1.0310\nEpoch 028 | Loss: 1.0187 | Val Loss: 1.0214\nEpoch 029 | Loss: 1.0181 | Val Loss: 1.0119\nEpoch 030 | Loss: 1.0122 | Val Loss: 1.0026\nEpoch 031 | Loss: 1.0019 | Val Loss: 0.9935\nEpoch 032 | Loss: 0.9943 | Val Loss: 0.9846\nEpoch 033 | Loss: 0.9889 | Val Loss: 0.9758\nEpoch 034 | Loss: 0.9749 | Val Loss: 0.9671\nEpoch 035 | Loss: 0.9713 | Val Loss: 0.9586\nEpoch 036 | Loss: 0.9646 | Val Loss: 0.9503\nEpoch 037 | Loss: 0.9562 | Val Loss: 0.9421\nEpoch 038 | Loss: 0.9454 | Val Loss: 0.9341\nEpoch 039 | Loss: 0.9459 | Val Loss: 0.9262\nEpoch 040 | Loss: 0.9344 | Val Loss: 0.9185\nEpoch 041 | Loss: 0.9284 | Val Loss: 0.9110\nEpoch 042 | Loss: 0.9125 | Val Loss: 0.9037\nEpoch 043 | Loss: 0.9209 | Val Loss: 0.8967\nEpoch 044 | Loss: 0.9151 | Val Loss: 0.8898\nEpoch 045 | Loss: 0.9009 | Val Loss: 0.8831\nEpoch 046 | Loss: 0.9066 | Val Loss: 0.8766\nEpoch 047 | Loss: 0.8930 | Val Loss: 0.8704\nEpoch 048 | Loss: 0.8801 | Val Loss: 0.8643\nEpoch 049 | Loss: 0.8773 | Val Loss: 0.8585\nEpoch 050 | Loss: 0.8802 | Val Loss: 0.8529\nEpoch 051 | Loss: 0.8707 | Val Loss: 0.8475\nEpoch 052 | Loss: 0.8715 | Val Loss: 0.8423\nEpoch 053 | Loss: 0.8626 | Val Loss: 0.8374\nEpoch 054 | Loss: 0.8607 | Val Loss: 0.8327\nEpoch 055 | Loss: 0.8549 | Val Loss: 0.8284\nEpoch 056 | Loss: 0.8447 | Val Loss: 0.8242\nEpoch 057 | Loss: 0.8501 | Val Loss: 0.8202\nEpoch 058 | Loss: 0.8482 | Val Loss: 0.8165\nEpoch 059 | Loss: 0.8455 | Val Loss: 0.8130\nEpoch 060 | Loss: 0.8427 | Val Loss: 0.8097\nEpoch 061 | Loss: 0.8311 | Val Loss: 0.8065\nEpoch 062 | Loss: 0.8319 | Val Loss: 0.8034\nEpoch 063 | Loss: 0.8237 | Val Loss: 0.8005\nEpoch 064 | Loss: 0.8314 | Val Loss: 0.7977\nEpoch 065 | Loss: 0.8193 | Val Loss: 0.7951\nEpoch 066 | Loss: 0.8153 | Val Loss: 0.7925\nEpoch 067 | Loss: 0.8113 | Val Loss: 0.7902\nEpoch 068 | Loss: 0.8127 | Val Loss: 0.7880\nEpoch 069 | Loss: 0.8030 | Val Loss: 0.7859\nEpoch 070 | Loss: 0.8119 | Val Loss: 0.7840\nEpoch 071 | Loss: 0.8025 | Val Loss: 0.7821\nEpoch 072 | Loss: 0.8009 | Val Loss: 0.7803\nEpoch 073 | Loss: 0.8046 | Val Loss: 0.7785\nEpoch 074 | Loss: 0.7999 | Val Loss: 0.7769\nEpoch 075 | Loss: 0.7952 | Val Loss: 0.7753\nEpoch 076 | Loss: 0.7932 | Val Loss: 0.7738\nEpoch 077 | Loss: 0.7890 | Val Loss: 0.7723\nEpoch 078 | Loss: 0.7868 | Val Loss: 0.7709\nEpoch 079 | Loss: 0.7891 | Val Loss: 0.7695\nEpoch 080 | Loss: 0.7853 | Val Loss: 0.7681\nEpoch 081 | Loss: 0.7902 | Val Loss: 0.7669\nEpoch 082 | Loss: 0.7755 | Val Loss: 0.7655\nEpoch 083 | Loss: 0.7878 | Val Loss: 0.7643\nEpoch 084 | Loss: 0.7763 | Val Loss: 0.7631\nEpoch 085 | Loss: 0.7702 | Val Loss: 0.7619\nEpoch 086 | Loss: 0.7769 | Val Loss: 0.7607\nEpoch 087 | Loss: 0.7642 | Val Loss: 0.7594\nEpoch 088 | Loss: 0.7733 | Val Loss: 0.7582\nEpoch 089 | Loss: 0.7613 | Val Loss: 0.7569\nEpoch 090 | Loss: 0.7716 | Val Loss: 0.7557\nEpoch 091 | Loss: 0.7650 | Val Loss: 0.7544\nEpoch 092 | Loss: 0.7759 | Val Loss: 0.7531\nEpoch 093 | Loss: 0.7692 | Val Loss: 0.7518\nEpoch 094 | Loss: 0.7742 | Val Loss: 0.7504\nEpoch 095 | Loss: 0.7634 | Val Loss: 0.7490\nEpoch 096 | Loss: 0.7535 | Val Loss: 0.7475\nEpoch 097 | Loss: 0.7540 | Val Loss: 0.7461\nEpoch 098 | Loss: 0.7572 | Val Loss: 0.7447\nEpoch 099 | Loss: 0.7505 | Val Loss: 0.7432\nEpoch 100 | Loss: 0.7517 | Val Loss: 0.7417\nEpoch 101 | Loss: 0.7516 | Val Loss: 0.7403\nEpoch 102 | Loss: 0.7554 | Val Loss: 0.7390\nEpoch 103 | Loss: 0.7533 | Val Loss: 0.7377\nEpoch 104 | Loss: 0.7444 | Val Loss: 0.7365\nEpoch 105 | Loss: 0.7473 | Val Loss: 0.7354\nEpoch 106 | Loss: 0.7328 | Val Loss: 0.7342\nEpoch 107 | Loss: 0.7344 | Val Loss: 0.7330\nEpoch 108 | Loss: 0.7452 | Val Loss: 0.7320\nEpoch 109 | Loss: 0.7347 | Val Loss: 0.7310\nEpoch 110 | Loss: 0.7424 | Val Loss: 0.7299\nEpoch 111 | Loss: 0.7326 | Val Loss: 0.7289\nEpoch 112 | Loss: 0.7411 | Val Loss: 0.7279\nEpoch 113 | Loss: 0.7390 | Val Loss: 0.7269\nEpoch 114 | Loss: 0.7300 | Val Loss: 0.7260\nEpoch 115 | Loss: 0.7291 | Val Loss: 0.7250\nEpoch 116 | Loss: 0.7303 | Val Loss: 0.7240\nEpoch 117 | Loss: 0.7333 | Val Loss: 0.7231\nEpoch 118 | Loss: 0.7350 | Val Loss: 0.7221\nEpoch 119 | Loss: 0.7316 | Val Loss: 0.7212\nEpoch 120 | Loss: 0.7285 | Val Loss: 0.7204\nEpoch 121 | Loss: 0.7149 | Val Loss: 0.7194\nEpoch 122 | Loss: 0.7235 | Val Loss: 0.7185\nEpoch 123 | Loss: 0.7106 | Val Loss: 0.7175\nEpoch 124 | Loss: 0.7227 | Val Loss: 0.7165\nEpoch 125 | Loss: 0.7140 | Val Loss: 0.7155\nEpoch 126 | Loss: 0.7205 | Val Loss: 0.7146\nEpoch 127 | Loss: 0.7191 | Val Loss: 0.7136\nEpoch 128 | Loss: 0.7262 | Val Loss: 0.7128\nEpoch 129 | Loss: 0.7204 | Val Loss: 0.7121\nEpoch 130 | Loss: 0.7106 | Val Loss: 0.7115\nEpoch 131 | Loss: 0.7079 | Val Loss: 0.7109\nEpoch 132 | Loss: 0.7168 | Val Loss: 0.7102\nEpoch 133 | Loss: 0.7100 | Val Loss: 0.7095\nEpoch 134 | Loss: 0.7132 | Val Loss: 0.7090\nEpoch 135 | Loss: 0.7158 | Val Loss: 0.7084\nEpoch 136 | Loss: 0.7112 | Val Loss: 0.7078\nEpoch 137 | Loss: 0.7115 | Val Loss: 0.7072\nEpoch 138 | Loss: 0.7097 | Val Loss: 0.7068\nEpoch 139 | Loss: 0.7083 | Val Loss: 0.7063\nEpoch 140 | Loss: 0.7014 | Val Loss: 0.7057\nEpoch 141 | Loss: 0.7023 | Val Loss: 0.7051\nEpoch 142 | Loss: 0.6925 | Val Loss: 0.7044\nEpoch 143 | Loss: 0.7181 | Val Loss: 0.7038\nEpoch 144 | Loss: 0.7013 | Val Loss: 0.7032\nEpoch 145 | Loss: 0.7060 | Val Loss: 0.7024\nEpoch 146 | Loss: 0.6996 | Val Loss: 0.7016\nEpoch 147 | Loss: 0.6976 | Val Loss: 0.7009\nEpoch 148 | Loss: 0.7079 | Val Loss: 0.7001\nEpoch 149 | Loss: 0.6969 | Val Loss: 0.6992\nEpoch 150 | Loss: 0.6974 | Val Loss: 0.6983\nEpoch 151 | Loss: 0.7002 | Val Loss: 0.6973\nEpoch 152 | Loss: 0.7001 | Val Loss: 0.6964\nEpoch 153 | Loss: 0.6842 | Val Loss: 0.6954\nEpoch 154 | Loss: 0.6885 | Val Loss: 0.6943\nEpoch 155 | Loss: 0.6815 | Val Loss: 0.6932\nEpoch 156 | Loss: 0.6823 | Val Loss: 0.6922\nEpoch 157 | Loss: 0.6927 | Val Loss: 0.6911\nEpoch 158 | Loss: 0.6868 | Val Loss: 0.6901\nEpoch 159 | Loss: 0.6912 | Val Loss: 0.6892\nEpoch 160 | Loss: 0.6803 | Val Loss: 0.6883\nEpoch 161 | Loss: 0.6839 | Val Loss: 0.6874\nEpoch 162 | Loss: 0.6851 | Val Loss: 0.6864\nEpoch 163 | Loss: 0.6715 | Val Loss: 0.6855\nEpoch 164 | Loss: 0.6837 | Val Loss: 0.6845\nEpoch 165 | Loss: 0.6821 | Val Loss: 0.6836\nEpoch 166 | Loss: 0.6713 | Val Loss: 0.6827\nEpoch 167 | Loss: 0.6789 | Val Loss: 0.6819\nEpoch 168 | Loss: 0.6688 | Val Loss: 0.6812\nEpoch 169 | Loss: 0.6762 | Val Loss: 0.6806\nEpoch 170 | Loss: 0.6760 | Val Loss: 0.6800\nEpoch 171 | Loss: 0.6836 | Val Loss: 0.6794\nEpoch 172 | Loss: 0.6763 | Val Loss: 0.6789\nEpoch 173 | Loss: 0.6746 | Val Loss: 0.6785\nEpoch 174 | Loss: 0.6772 | Val Loss: 0.6779\nEpoch 175 | Loss: 0.6722 | Val Loss: 0.6774\nEpoch 176 | Loss: 0.6751 | Val Loss: 0.6768\nEpoch 177 | Loss: 0.6668 | Val Loss: 0.6761\nEpoch 178 | Loss: 0.6615 | Val Loss: 0.6754\nEpoch 179 | Loss: 0.6624 | Val Loss: 0.6748\nEpoch 180 | Loss: 0.6669 | Val Loss: 0.6741\nEpoch 181 | Loss: 0.6668 | Val Loss: 0.6734\nEpoch 182 | Loss: 0.6704 | Val Loss: 0.6728\nEpoch 183 | Loss: 0.6662 | Val Loss: 0.6723\nEpoch 184 | Loss: 0.6686 | Val Loss: 0.6717\nEpoch 185 | Loss: 0.6583 | Val Loss: 0.6711\nEpoch 186 | Loss: 0.6597 | Val Loss: 0.6704\nEpoch 187 | Loss: 0.6630 | Val Loss: 0.6697\nEpoch 188 | Loss: 0.6634 | Val Loss: 0.6692\nEpoch 189 | Loss: 0.6588 | Val Loss: 0.6686\nEpoch 190 | Loss: 0.6604 | Val Loss: 0.6680\nEpoch 191 | Loss: 0.6514 | Val Loss: 0.6674\nEpoch 192 | Loss: 0.6535 | Val Loss: 0.6669\nEpoch 193 | Loss: 0.6542 | Val Loss: 0.6665\nEpoch 194 | Loss: 0.6420 | Val Loss: 0.6662\nEpoch 195 | Loss: 0.6577 | Val Loss: 0.6661\nEpoch 196 | Loss: 0.6523 | Val Loss: 0.6660\nEpoch 197 | Loss: 0.6486 | Val Loss: 0.6659\nEpoch 198 | Loss: 0.6428 | Val Loss: 0.6658\nEpoch 199 | Loss: 0.6627 | Val Loss: 0.6655\nEpoch 200 | Loss: 0.6584 | Val Loss: 0.6653\n\nðŸŽ¯ Best Threshold = 0.784\nPrecision=0.786, Recall=0.650, F1=0.711\nROC-AUC: 0.9119189330455748\n              precision    recall  f1-score   support\n\n           0       0.94      0.97      0.95      1401\n           1       0.78      0.65      0.71       254\n\n    accuracy                           0.92      1655\n   macro avg       0.86      0.81      0.83      1655\nweighted avg       0.91      0.92      0.92      1655\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --------- Extract static embeddings from DNN (FULL DATASET) ---------\n\nmodel.eval()\n\ndef get_embeddings(model, X):\n    with torch.no_grad():\n        _, features = model(torch.tensor(X, dtype=torch.float32))\n    return features.numpy()\n\n# Extract embeddings for ALL rows\nstatic_embeddings = get_embeddings(model, X)\n\n# Save to CSV\nimport pandas as pd\n\ndf_emb = pd.DataFrame(\n    static_embeddings,\n    columns=[f\"static_emb_{i}\" for i in range(static_embeddings.shape[1])]\n)\n\ndf_emb = pd.concat([df[['stay_id']].reset_index(drop=True), df_emb], axis=1)\ndf_emb.to_csv(\"static_embeddings.csv\", index=False)\n\nprint(\"ðŸ”¥ Saved static embeddings.csv\")\nprint(\"Shape:\", static_embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:32:28.730506Z","iopub.execute_input":"2025-11-13T06:32:28.730949Z","iopub.status.idle":"2025-11-13T06:32:28.950007Z","shell.execute_reply.started":"2025-11-13T06:32:28.730929Z","shell.execute_reply":"2025-11-13T06:32:28.949340Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¥ Saved static embeddings.csv\nShape: (8274, 32)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import joblib\nimport torch\n\n# --- Save preprocessing objects ---\njoblib.dump(encoder, \"encoder.pkl\")\njoblib.dump(scaler, \"scaler.pkl\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T06:32:57.179048Z","iopub.execute_input":"2025-11-13T06:32:57.179792Z","iopub.status.idle":"2025-11-13T06:32:57.186223Z","shell.execute_reply.started":"2025-11-13T06:32:57.179765Z","shell.execute_reply":"2025-11-13T06:32:57.185675Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['scaler.pkl']"},"metadata":{}}],"execution_count":8}]}