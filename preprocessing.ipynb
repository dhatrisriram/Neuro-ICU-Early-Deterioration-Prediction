{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13383139,"sourceType":"datasetVersion","datasetId":8491501}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\nprint(\"Using device:\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:19:38.346101Z","iopub.execute_input":"2025-10-19T09:19:38.346912Z","iopub.status.idle":"2025-10-19T09:19:42.603400Z","shell.execute_reply.started":"2025-10-19T09:19:38.346879Z","shell.execute_reply":"2025-10-19T09:19:42.602591Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_79/2669706378.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Robust ventilation detection (<= 12 h) using d_items to select vent-specific itemids\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n\nICU_PATH = \"/kaggle/input/neuro-icu/icu/icu/\"\n\n# -------------------------\n# 1) load icu stays (neuro)\n# -------------------------\nicu = pd.read_csv(\n    ICU_PATH + \"icustays.csv\",\n    usecols=[\"stay_id\", \"hadm_id\", \"subject_id\", \"first_careunit\", \"intime\", \"outtime\", \"los\"]\n)\nneuro_units = [\"Neuro Intermediate\", \"Neuro Stepdown\", \"Neuro Surgical Intensive Care Unit\"]\nneuro_icu = icu[icu[\"first_careunit\"].isin(neuro_units)].copy()\nneuro_icu[\"intime\"] = pd.to_datetime(neuro_icu[\"intime\"])\nneuro_stays = set(neuro_icu[\"stay_id\"].unique())\nprint(f\"Total Neuro-ICU stays: {len(neuro_stays)}\")\n\n# -------------------------\n# 2) inspect d_items for vent-specific labels\n# -------------------------\nd_items = pd.read_csv(ICU_PATH + \"d_items.csv\", usecols=[\"itemid\",\"label\",\"category\",\"param_type\"], low_memory=False)\nd_items[\"label_l\"] = d_items[\"label\"].str.lower().fillna(\"\")\n\n# ventilation-identifying keywords (conservative)\nvent_keywords = [\n    r\"ventil\",        # ventilator, ventilation, ventilatory\n    r\"endotracheal\",  # endotracheal tube / ETT\n    r\"ett\\b\",         # ETT abbreviation (word boundary)\n    r\"intubat\",       # intubation / intubated\n    r\"tracheostom\",   # tracheostomy\n    r\"vent mode\",     # ventilator mode\n    r\"ventilator\",    # explicit ventilator labels\n    r\"airway device\", # some datasets\n    r\"mechanical vent\" # mechanical ventilation phrase\n]\n\n# build regex\nvent_regex = re.compile(\"|\".join(vent_keywords), flags=re.IGNORECASE)\n\n# conservative filter: keep entries where label strongly indicates ventilator/device/intubation\nd_items[\"is_vent_label\"] = d_items[\"label\"].str.contains(vent_regex, na=False)\nvent_itemids_from_dict = set(d_items.loc[d_items[\"is_vent_label\"], \"itemid\"].unique())\nprint(f\"Vent-related itemids found in d_items (conservative): {len(vent_itemids_from_dict)}\")\n\n# show the matched labels to inspect (top 50)\nmatched_labels = d_items.loc[d_items[\"is_vent_label\"], [\"itemid\",\"label\"]].drop_duplicates().sort_values(\"itemid\")\nprint(\"Sample matched vent-related d_items rows (inspect these):\")\nprint(matched_labels.head(50).to_string(index=False))\n\n# -------------------------\n# 3) procedureevents (explicit)\n# -------------------------\nproc_cols = [\"stay_id\",\"ordercategoryname\",\"starttime\"]\nprocedures = pd.read_csv(ICU_PATH + \"procedureevents.csv\", usecols=proc_cols, low_memory=False)\nprocedures = procedures[procedures[\"stay_id\"].isin(neuro_stays)].copy()\nprocedures[\"starttime\"] = pd.to_datetime(procedures[\"starttime\"], errors=\"coerce\")\nprocedures = procedures.merge(neuro_icu[[\"stay_id\",\"intime\"]], on=\"stay_id\", how=\"left\")\nprocedures[\"hours_from_admit\"] = (procedures[\"starttime\"] - procedures[\"intime\"]).dt.total_seconds()/3600\n\nproc_mask = procedures[\"ordercategoryname\"].str.contains(r\"Ventilation|Intubation|Extubation\", flags=re.IGNORECASE, regex=True, na=False)\nvent_stays_proc = set(procedures.loc[proc_mask & (procedures[\"hours_from_admit\"] <= 12), \"stay_id\"].unique())\nprint(f\"Ventilation within 12 h (procedureevents): {len(vent_stays_proc)}\")\n\n# -------------------------\n# 4) chartevents â€” only vent-specific itemids from d_items\n# -------------------------\n# If no vent itemids found (rare), fall back to small known safe list; else use discovered list\nif len(vent_itemids_from_dict) == 0:\n    # fallback safe list (very conservative; likely small)\n    vent_itemids = {220339, 224685}  # ventilator mode/type - fallback\n    print(\"WARNING: no vent itemids found in d_items; using fallback list:\", vent_itemids)\nelse:\n    vent_itemids = vent_itemids_from_dict\n\nvent_stays_chart = set()\n# also track which itemids contributed most so we can inspect\nchart_itemid_counts = {}\n\nusecols_chart = [\"stay_id\",\"charttime\",\"itemid\",\"value\",\"valueuom\"]\nfor chunk in tqdm(pd.read_csv(ICU_PATH + \"chartevents.csv\", usecols=usecols_chart, chunksize=1_000_000, low_memory=False)):\n    chunk = chunk[chunk[\"stay_id\"].isin(neuro_stays)]\n    if chunk.empty:\n        continue\n    # filter to vent-specific itemids\n    chunk = chunk[chunk[\"itemid\"].isin(vent_itemids)]\n    if chunk.empty:\n        continue\n    chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n    chunk = chunk.merge(neuro_icu[[\"stay_id\",\"intime\"]], on=\"stay_id\", how=\"left\")\n    chunk[\"hours_from_admit\"] = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds()/3600\n    # keep only <= 12 hrs\n    chunk = chunk[chunk[\"hours_from_admit\"].between(0, 12)]\n    if chunk.empty:\n        continue\n    # update stay ids\n    vent_stays_chart.update(chunk[\"stay_id\"].unique())\n    # count itemid contributions\n    counts = chunk[\"itemid\"].value_counts().to_dict()\n    for k,v in counts.items():\n        chart_itemid_counts[k] = chart_itemid_counts.get(k,0) + int(v)\n\nprint(f\"Ventilation indicators (chartevents <=12 h) using vent-specific d_items: {len(vent_stays_chart)}\")\n\n# Print top contributing itemids + labels for inspection\ntop_items = sorted(chart_itemid_counts.items(), key=lambda x: x[1], reverse=True)[:30]\nif top_items:\n    print(\"\\nTop chartevent itemids that matched (itemid -> count -> label):\")\n    for itemid,count in top_items:\n        label = d_items.loc[d_items[\"itemid\"]==itemid, \"label\"].iloc[0] if any(d_items[\"itemid\"]==itemid) else \"\"\n        print(f\"  {itemid} -> {count} times -> {label}\")\n# -------------------------\n# 5) inputevents â€” airway / paralytic / sedation orders (proxies, optimized with d_items)\n# -------------------------\n\n# Define patterns for sedation / airway / ventilation drugs and procedures\nvent_input_keywords = [\n    r\"ventil\", r\"airway\", r\"intubat\", r\"ett\", r\"endotracheal\", r\"tracheostom\",\n    r\"respirat\", r\"sedation\", r\"sedative\", r\"anesth\", r\"paralytic\",\n    r\"propofol\", r\"midazolam\", r\"lorazepam\", r\"diazepam\",\n    r\"fentanyl\", r\"morphine\", r\"rocuronium\", r\"vecuronium\",\n    r\"succinylcholine\", r\"etomidate\"\n]\nvent_input_regex = re.compile(\"|\".join(vent_input_keywords), flags=re.IGNORECASE)\n\n# --- Step 1: Identify relevant input itemids from d_items ---\nd_items[\"label_l\"] = d_items[\"label\"].str.lower().fillna(\"\")\nvent_input_itemids = set(\n    d_items.loc[d_items[\"label_l\"].str.contains(vent_input_regex, na=False), \"itemid\"].unique()\n)\nprint(f\"Found {len(vent_input_itemids)} potential vent/sedation itemids in d_items\")\n\n# --- Step 2: Process inputevents efficiently ---\ninput_cols = [\n    \"stay_id\",\"itemid\",\"starttime\",\"ordercategoryname\",\n    \"secondaryordercategoryname\",\"ordercategorydescription\",\n    \"ordercomponenttypedescription\",\"statusdescription\"\n]\n\nvent_stays_inputs = set()\nchart_itemid_counts = {}\n\nfor chunk in tqdm(pd.read_csv(ICU_PATH + \"inputevents.csv\", usecols=input_cols, chunksize=500_000, low_memory=False)):\n    chunk = chunk[chunk[\"stay_id\"].isin(neuro_stays)]\n    if chunk.empty:\n        continue\n\n    # Filter by itemid first (fast path)\n    chunk = chunk[chunk[\"itemid\"].isin(vent_input_itemids)]\n    if chunk.empty:\n        # fallback to text search (slow path)\n        combined_text = (\n            chunk[\"ordercategoryname\"].fillna(\"\") + \" \" +\n            chunk[\"secondaryordercategoryname\"].fillna(\"\") + \" \" +\n            chunk[\"ordercategorydescription\"].fillna(\"\") + \" \" +\n            chunk[\"ordercomponenttypedescription\"].fillna(\"\") + \" \" +\n            chunk[\"statusdescription\"].fillna(\"\")\n        )\n        chunk = chunk[combined_text.str.contains(vent_input_regex, na=False)]\n    if chunk.empty:\n        continue\n\n    # Time filter (<= 12 h from ICU admit)\n    chunk[\"starttime\"] = pd.to_datetime(chunk[\"starttime\"], errors=\"coerce\")\n    chunk = chunk.merge(neuro_icu[[\"stay_id\",\"intime\"]], on=\"stay_id\", how=\"left\")\n    chunk[\"hours_from_admit\"] = (chunk[\"starttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600\n    valid_stays = chunk.loc[chunk[\"hours_from_admit\"].between(0,12), \"stay_id\"].unique()\n\n    vent_stays_inputs.update(valid_stays)\n\n    # track which itemids contributed (for debugging)\n    counts = chunk[\"itemid\"].value_counts().to_dict()\n    for k,v in counts.items():\n        chart_itemid_counts[k] = chart_itemid_counts.get(k,0) + int(v)\n\nprint(f\"âœ… Ventilation-related inputevents (<=12 h): {len(vent_stays_inputs)}\")\n\n# Optional: inspect top contributing itemids\nif chart_itemid_counts:\n    print(\"\\nTop contributing input itemids:\")\n    for itemid,count in sorted(chart_itemid_counts.items(), key=lambda x: x[1], reverse=True)[:20]:\n        label = d_items.loc[d_items[\"itemid\"] == itemid, \"label\"].iloc[0] if any(d_items[\"itemid\"] == itemid) else \"\"\n        print(f\"  {itemid} -> {count} times -> {label}\")\n\n\n\n# -------------------------\n# 6) Combine and save\n# -------------------------\nvent_all = vent_stays_proc.union(vent_stays_chart).union(vent_stays_inputs)\nprint(\"\\n============================\")\nprint(f\"Total unique ventilated stays (<=12 h): {len(vent_all)} / {len(neuro_stays)}\")\nprint(f\"Class ratio: {len(vent_all)/len(neuro_stays):.2%}\")\nprint(\"============================\")\n\n# Save CSV with label = 1 for ventilated within 12h\npd.DataFrame({\"stay_id\": list(vent_all), \"ventilated_12h\":1}).to_csv(\"ventilation_labels_12h.csv\", index=False)\nprint(\"Saved ventilation_labels_12h.csv\")\n\n# Also save per-source stay lists for inspection\npd.DataFrame({\"stay_id\": list(vent_stays_proc)}).to_csv(\"vent_proc_12h.csv\", index=False)\npd.DataFrame({\"stay_id\": list(vent_stays_chart)}).to_csv(\"vent_chart_12h.csv\", index=False)\npd.DataFrame({\"stay_id\": list(vent_stays_inputs)}).to_csv(\"vent_inputs_12h.csv\", index=False)\nprint(\"Saved vent_proc_12h.csv, vent_chart_12h.csv, vent_inputs_12h.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:19:46.133656Z","iopub.execute_input":"2025-10-19T09:19:46.134305Z","iopub.status.idle":"2025-10-19T09:37:06.993215Z","shell.execute_reply.started":"2025-10-19T09:19:46.134283Z","shell.execute_reply":"2025-10-19T09:37:06.992434Z"}},"outputs":[{"name":"stdout","text":"Total Neuro-ICU stays: 7197\nVent-related itemids found in d_items (conservative): 81\nSample matched vent-related d_items rows (inspect these):\n itemid                                                label\n 222871               Mallampati classification (Intubation)\n 223059                                 Intubation - Details\n 223837                                        ETT Size (ID)\n 223838                                         ETT Location\n 223840                                         ETT Re-taped\n 223848                                      Ventilator Type\n 223849                                      Ventilator Mode\n 224385                                           Intubation\n 224391                                  ETT Mark (location)\n 224392                                  ETT Position Change\n 224415                                        ETT Mark (cm)\n 224832                                             ETT Type\n 225267                                Difficult to Intubate\n 225271                   Induction/Drugs/Doses (Intubation)\n 225277                           ETT Position (cm at teeth)\n 225278                                            ETT Route\n 225291                              Indication (Intubation)\n 225292    Unable to complete Airway Assessment (Intubation)\n 225294                           Mouth Opening (Intubation)\n 225295                    Thyromental Distance (Intubation)\n 225296                   Mandibular Prognatism (Intubation)\n 225297                         Teeth/Dentition (Intubation)\n 225298                                Neck ROM (Intubation)\n 225300                   No Drugs Administered (Intubation)\n 225301                                     RSI (Intubation)\n 225302                         Pre-Oxygentated (Intubation)\n 225303                        Mask Ventilation (Intubation)\n 225304                       Blade / Equipment (Intubation)\n 225306                      Number of Attempts (Intubation)\n 225307                                             Oral ETT\n 225308                                            Nasal ETT\n 225448                            Percutaneous Tracheostomy\n 225585                               ETT Depth (Intubation)\n 225586                                Stylette (Intubation)\n 225587                                  Bougie (Intubation)\n 225588                            Tube Secured (Intubation)\n 225590                   Tracheal Confirmation (Intubation)\n 225592                                ETT Size (Intubation)\n 225593               Best Larynx View Obtained (Intubation)\n 225792                                 Invasive Ventilation\n 225794                             Non-invasive Ventilation\n 226166                        No Complications (Intubation)\n 226188                    Timeout Performed by (Intubation)\n 226189 Patient identified correctly by 2 means (Intubation)\n 226190       Hand Cleansing prior to procedure (Intubation)\n 226191            Barrier precautions in place (Intubation)\n 226237                                    Open Tracheostomy\n 226260                              Mechanically Ventilated\n 226429                                ETT Type (Intubation)\n 226430                           Complications (Intubation)\nVentilation within 12 h (procedureevents): 538\n","output_type":"stream"},{"name":"stderr","text":"433it [16:12,  2.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Ventilation indicators (chartevents <=12 h) using vent-specific d_items: 703\n\nTop chartevent itemids that matched (itemid -> count -> label):\n  223848 -> 2180 times -> Ventilator Type\n  224415 -> 2016 times -> ETT Mark (cm)\n  229314 -> 2002 times -> Ventilator Mode (Hamilton)\n  223837 -> 1966 times -> ETT Size (ID)\n  223838 -> 1965 times -> ETT Location\n  224832 -> 1954 times -> ETT Type\n  226814 -> 1434 times -> Known difficult intubation\n  224391 -> 1203 times -> ETT Mark (location)\n  227566 -> 521 times -> Ventilator Tank #2\n  227565 -> 519 times -> Ventilator Tank #1\n  223849 -> 459 times -> Ventilator Mode\n  227809 -> 192 times -> ETT Position Change\n  227810 -> 40 times -> ETT Re-taped\n  230045 -> 32 times -> Intellivent (Hamilton Vent Mode)\nFound 132 potential vent/sedation itemids in d_items\n","output_type":"stream"},{"name":"stderr","text":"22it [01:04,  2.91s/it]","output_type":"stream"},{"name":"stdout","text":"âœ… Ventilation-related inputevents (<=12 h): 803\n\nTop contributing input itemids:\n  222168 -> 16014 times -> Propofol\n  221744 -> 3797 times -> Fentanyl\n  225942 -> 2161 times -> Fentanyl (Concentrate)\n  225154 -> 1847 times -> Morphine Sulfate\n  221668 -> 1647 times -> Midazolam (Versed)\n  221385 -> 960 times -> Lorazepam (Ativan)\n  229233 -> 100 times -> Rocuronium\n  221623 -> 61 times -> Diazepam (Valium)\n  222062 -> 1 times -> Vecuronium\n\n============================\nTotal unique ventilated stays (<=12 h): 993 / 7197\nClass ratio: 13.80%\n============================\nSaved ventilation_labels_12h.csv\nSaved vent_proc_12h.csv, vent_chart_12h.csv, vent_inputs_12h.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# 1. Preprocess ICU Vitals, Inputs, Outputs (first 6h)\n# ============================================================\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n\nICU_PATH = \"/kaggle/input/neuro-icu/icu/icu/\"\n\n# neuro_icu and neuro_stays should already exist from Cell 2\n# ------------------------------------------------------------\n\n# ============================================================\n# 2. Chunked Load for Chartevents (Vitals, first 6h)\n# ============================================================\nvital_chunks = []\nfor chunk in tqdm(pd.read_csv(\n    ICU_PATH + \"chartevents.csv\",\n    usecols=[\"stay_id\", \"charttime\", \"itemid\", \"valuenum\"],\n    chunksize=1_000_000, low_memory=False\n)):\n    chunk = chunk[chunk[\"stay_id\"].isin(neuro_stays)]\n    if chunk.empty:\n        continue\n    chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n    chunk = chunk.merge(neuro_icu[[\"stay_id\", \"intime\"]], on=\"stay_id\", how=\"left\")\n    chunk[\"hours_from_admit\"] = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600\n    chunk = chunk[chunk[\"hours_from_admit\"].between(0, 6)]\n    vital_chunks.append(chunk)\n\nchart = pd.concat(vital_chunks, ignore_index=True)\nprint(f\"Vitals records (first 6h): {len(chart):,}\")\n\n# ============================================================\n# 3. Chunked Load for Inputevents (Meds/Fluids)\n# ============================================================\ninput_chunks = []\nfor chunk in tqdm(pd.read_csv(\n    ICU_PATH + \"inputevents.csv\",\n    usecols=[\"stay_id\", \"starttime\", \"amount\", \"ordercategoryname\"],\n    chunksize=500_000, low_memory=False\n)):\n    chunk = chunk[chunk[\"stay_id\"].isin(neuro_stays)]\n    if chunk.empty:\n        continue\n    chunk[\"starttime\"] = pd.to_datetime(chunk[\"starttime\"], errors=\"coerce\")\n    chunk = chunk.merge(neuro_icu[[\"stay_id\", \"intime\"]], on=\"stay_id\", how=\"left\")\n    chunk[\"hours_from_admit\"] = (chunk[\"starttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600\n    chunk = chunk[chunk[\"hours_from_admit\"].between(0, 6)]\n    input_chunks.append(chunk)\n\ninputs = pd.concat(input_chunks, ignore_index=True)\nprint(f\"Inputs (first 6h): {len(inputs):,}\")\n\n# ============================================================\n# 4. Chunked Load for Outputevents\n# ============================================================\noutput_chunks = []\nfor chunk in tqdm(pd.read_csv(\n    ICU_PATH + \"outputevents.csv\",\n    usecols=[\"stay_id\", \"charttime\", \"itemid\", \"value\"],\n    chunksize=500_000, low_memory=False\n)):\n    chunk = chunk[chunk[\"stay_id\"].isin(neuro_stays)]\n    if chunk.empty:\n        continue\n    chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n    chunk = chunk.merge(neuro_icu[[\"stay_id\", \"intime\"]], on=\"stay_id\", how=\"left\")\n    chunk[\"hours_from_admit\"] = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600\n    chunk = chunk[chunk[\"hours_from_admit\"].between(0, 6)]\n    output_chunks.append(chunk)\n\noutputs = pd.concat(output_chunks, ignore_index=True)\nprint(f\"Outputs (first 6h): {len(outputs):,}\")\n\n# ============================================================\n# 6. Save processed files\n# ============================================================\nchart.to_csv(\"processed_vitals_6h.csv\", index=False)\ninputs.to_csv(\"processed_inputs_6h.csv\", index=False)\noutputs.to_csv(\"processed_outputs_6h.csv\", index=False)\n\nprint(\"\\nâœ… Memory-safe preprocessing complete! Files saved:\")\nprint(\"- processed_vitals_6h.csv\")\nprint(\"- processed_inputs_6h.csv\")\nprint(\"- processed_outputs_6h.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T09:41:53.058743Z","iopub.execute_input":"2025-10-19T09:41:53.059123Z","iopub.status.idle":"2025-10-19T09:57:39.462284Z","shell.execute_reply.started":"2025-10-19T09:41:53.059101Z","shell.execute_reply":"2025-10-19T09:57:39.461407Z"}},"outputs":[{"name":"stderr","text":"433it [14:25,  2.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Vitals records (first 6h): 2,982,925\n","output_type":"stream"},{"name":"stderr","text":"22it [00:53,  2.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Inputs (first 6h): 26,978\n","output_type":"stream"},{"name":"stderr","text":"11it [00:09,  1.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Outputs (first 6h): 14,974\n\nâœ… Memory-safe preprocessing complete! Files saved:\n- processed_vitals_6h.csv\n- processed_inputs_6h.csv\n- processed_outputs_6h.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# 1. Import Libraries\n# ============================================================\nimport pandas as pd\nimport numpy as np\n\n# ============================================================\n# 2. Load Preprocessed Files (6-hour data)\n# ============================================================\nchart_6h = pd.read_csv(\"processed_vitals_6h.csv\")       # vitals from first 6h\ninputs = pd.read_csv(\"processed_inputs_6h.csv\")          # meds/fluids\noutputs = pd.read_csv(\"processed_outputs_6h.csv\")        # urine/drains\n\nprint(f\"Vitals (6h): {len(chart_6h):,}\")\nprint(f\"Inputs (6h): {len(inputs):,}\")\nprint(f\"Outputs (6h): {len(outputs):,}\")\n\n# ============================================================\n# 3. Aggregate Vitals into Hourly Trends per Patient\n# ============================================================\nchart_6h[\"hour_bin\"] = chart_6h[\"hours_from_admit\"].astype(int)\nvital_agg = (\n    chart_6h.groupby([\"stay_id\", \"hour_bin\"])[\"valuenum\"]\n    .agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n    .reset_index()\n)\n\n# Aggregate across the 6-hour window per patient\nvital_summary = (\n    vital_agg.groupby(\"stay_id\")\n    .agg({\n        \"mean\": \"mean\",\n        \"std\": \"mean\",\n        \"min\": \"min\",\n        \"max\": \"max\",\n        \"count\": \"sum\"\n    })\n    .reset_index()\n)\nvital_summary.columns = [\n    \"stay_id\",\n    \"vital_mean\",\n    \"vital_std\",\n    \"vital_min\",\n    \"vital_max\",\n    \"vital_count\"\n]\n\n# ============================================================\n# 4. Aggregate Inputs and Outputs (Meds, Fluids, Urine)\n# ============================================================\ninput_summary = (\n    inputs.groupby(\"stay_id\")\n    .agg(\n        total_input_ml=(\"amount\", \"sum\"),\n    )\n    .reset_index()\n)\n\noutput_summary = (\n    outputs.groupby(\"stay_id\")\n    .agg(total_output_ml=(\"value\", \"sum\"))\n    .reset_index()\n)\n\n# ============================================================\n# Add ventilation labels (<=12h)\n# ============================================================\nvent_labels = pd.read_csv(\"ventilation_labels_12h.csv\")\n\n# Merge with neuro_icu or your main patient summary to create label\nneuro_icu = neuro_icu.merge(vent_labels, on=\"stay_id\", how=\"left\")\nneuro_icu[\"ventilation_within_12h\"] = neuro_icu[\"ventilated_12h\"].fillna(0).astype(int)\nneuro_icu.drop(columns=[\"ventilated_12h\"], inplace=True)\n\n# ============================================================\n# 5. Merge All Modalities into Unified Modeling Dataset\n# ============================================================\nmodel_df = (\n    neuro_icu.merge(vital_summary, on=\"stay_id\", how=\"left\")\n             .merge(input_summary, on=\"stay_id\", how=\"left\")\n             .merge(output_summary, on=\"stay_id\", how=\"left\")\n)\n\nprint(\"Label (ventilation_within_12h) positive count:\",\n      model_df[\"ventilation_within_12h\"].sum())\n\n# Replace impossible values for vitals with median (more realistic)\nfor col in [\"vital_mean\", \"vital_std\", \"vital_min\", \"vital_max\"]:\n    model_df[col] = model_df[col].fillna(model_df[col].median())\n\n# Replace event-style features with 0 (absence = none)\nfor col in [\"vital_count\", \"total_input_ml\", \"total_output_ml\"]:\n    model_df[col] = model_df[col].fillna(0)\n\n\n# ============================================================\n# 6. Sanity Check and Label Distribution\n# ============================================================\nprint(\"\\nâœ… Aggregated 6-hour modeling dataset created.\")\nprint(\"Shape:\", model_df.shape)\nprint(\"Columns:\", list(model_df.columns))\nprint(\"Label (ventilation_within_12h) positive count:\",\n      model_df[\"ventilation_within_12h\"].sum())\n\n# ============================================================\n# 7. Save Final Modeling Dataset\n# ============================================================\nmodel_df.to_csv(\"model_ready_neuro_6h.csv\", index=False)\nprint(\"\\nðŸ’¾ Saved as: model_ready_neuro_6h.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T10:03:10.743905Z","iopub.execute_input":"2025-10-19T10:03:10.744550Z","iopub.status.idle":"2025-10-19T10:03:13.763885Z","shell.execute_reply.started":"2025-10-19T10:03:10.744526Z","shell.execute_reply":"2025-10-19T10:03:13.763047Z"}},"outputs":[{"name":"stdout","text":"Vitals (6h): 2,982,925\nInputs (6h): 26,978\nOutputs (6h): 14,974\nLabel (ventilation_within_12h) positive count: 993\n\nâœ… Aggregated 6-hour modeling dataset created.\nShape: (7197, 15)\nColumns: ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'intime', 'outtime', 'los', 'ventilation_within_12h', 'vital_mean', 'vital_std', 'vital_min', 'vital_max', 'vital_count', 'total_input_ml', 'total_output_ml']\nLabel (ventilation_within_12h) positive count: 993\n\nðŸ’¾ Saved as: model_ready_neuro_6h.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# 1. Import dependencies\n# ============================================================\nimport pandas as pd\nimport re\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# ============================================================\n# 2. Define text cleaning function\n# ============================================================\ndef clean_text(text):\n    \"\"\"\n    Cleans raw clinical note text:\n    - Lowercases\n    - Removes non-alphanumeric chars (except key punctuation)\n    - Normalizes spaces\n    \"\"\"\n    text = str(text).lower()\n    text = re.sub(r'\\n+', ' ', text)\n    text = re.sub(r'[^a-z0-9.,;:()/%\\- ]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n\n# ============================================================\n# 3. Helper: Process notes in chunks (memory-safe)\n# ============================================================\ndef process_notes_in_chunks(path, neuro_hadm_ids, id_col=\"hadm_id\", text_col=None, chunksize=50000):\n    \"\"\"\n    Streams large CSV files in chunks:\n    - Filters early by relevant hadm_ids\n    - Cleans text incrementally\n    - Returns combined dataframe of hadm_id + clean_text\n    \"\"\"\n    out = []\n    for chunk in tqdm(pd.read_csv(path, low_memory=False, chunksize=chunksize), desc=f\"Processing {path}\"):\n        if text_col is None:\n            text_col = chunk.columns[-1]  # assume last column contains text\n        chunk = chunk[chunk[id_col].isin(neuro_hadm_ids)]\n        if chunk.empty:\n            continue\n        chunk[\"clean_text\"] = chunk[text_col].apply(clean_text)\n        out.append(chunk[[id_col, \"clean_text\"]])\n    if not out:\n        return pd.DataFrame(columns=[id_col, \"clean_text\"])\n    return pd.concat(out, ignore_index=True)\n\n\n# ============================================================\n# 4. Load Neuro ICU cohort info\n# ============================================================\nicu = pd.read_csv(\"/kaggle/input/neuro-icu/icu/icu/icustays.csv\", usecols=[\"stay_id\", \"hadm_id\", \"first_careunit\"])\nneuro_units = [\"Neuro Intermediate\", \"Neuro Stepdown\", \"Neuro Surgical Intensive Care Unit\"]\nneuro_icu = icu[icu[\"first_careunit\"].isin(neuro_units)]\nneuro_hadm_ids = neuro_icu[\"hadm_id\"].unique().tolist()\n\n\n# ============================================================\n# 5. Process Discharge and Radiology Notes\n# ============================================================\nNOTE_DIR = \"/kaggle/input/neuro-icu/note/note/\"\n\ndischarge_clean = process_notes_in_chunks(f\"{NOTE_DIR}discharge.csv\", neuro_hadm_ids)\nradiology_clean = process_notes_in_chunks(f\"{NOTE_DIR}radiology.csv\", neuro_hadm_ids, text_col=\"text\")\n\nnotes = pd.concat([discharge_clean, radiology_clean], ignore_index=True)\nnotes = notes.drop_duplicates(subset=[\"hadm_id\", \"clean_text\"])\nprint(f\"ðŸ§¾ Total Neuro ICU admissions with notes: {notes['hadm_id'].nunique()}\")\n\n\n# ============================================================\n# 6. Condition Tagging (regex patterns)\n# ============================================================\nconditions = {\n    \"Ischemic_Stroke\": r\"\\bstroke\\b|\\bcerebral infarct\\b|\\bischemic\\b\",\n    \"Hemorrhage\": r\"\\bhemorrhage\\b|\\bbleed\\b|\\bICH\\b|\\bSAH\\b\",\n    \"Seizure\": r\"\\bseizure\\b|\\bconvulsion\\b|\\bstatus epilepticus\\b\",\n    \"Brain_Tumor\": r\"\\btumor\\b|\\bneoplasm\\b|\\bmeningioma\\b|\\bglioblastoma\\b\",\n    \"Hydrocephalus\": r\"\\bhydrocephalus\\b|\\bventriculomegaly\\b\",\n}\n\nfor cond, pattern in conditions.items():\n    notes[cond] = notes[\"clean_text\"].str.contains(pattern, flags=re.IGNORECASE, regex=True, na=False).astype(int)\n\n\n# ============================================================\n# 7. Collapse to per-admission level (avoid double counting)\n# ============================================================\n# Aggregate to 1 row per hadm_id â†’ any mention in any note = 1\nadm_conditions = notes.groupby(\"hadm_id\")[list(conditions.keys())].max().reset_index()\n\n# Merge with neuro ICU reference\nnotes_merged = neuro_icu.merge(adm_conditions, on=\"hadm_id\", how=\"left\").fillna(0)\n\n# Ensure integer dtype\nfor c in conditions.keys():\n    notes_merged[c] = notes_merged[c].astype(int)\n\n\n# ============================================================\n# 8. Save Cleaned + Tagged Notes (per admission)\n# ============================================================\nnotes_merged.to_csv(\"clean_neuro_notes_tagged.csv\", index=False)\nprint(\"âœ… Saved clean + tagged Neuro ICU notes â†’ 'clean_neuro_notes_tagged.csv'\")\n\n\n# ============================================================\n# 9. Summary Statistics\n# ============================================================\navg_len = notes[\"clean_text\"].str.len().mean()\ncondition_counts = notes_merged[list(conditions.keys())].sum().sort_values(ascending=False)\n\nprint(f\"\\nðŸ“Š Average note length (chars): {avg_len:.0f}\")\nprint(\"ðŸ§© Condition distribution (unique admissions):\\n\", condition_counts)\nprint(f\"\\nðŸ§¾ Total Neuro ICU admissions: {notes_merged['hadm_id'].nunique()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T10:19:44.486250Z","iopub.execute_input":"2025-10-19T10:19:44.487073Z","iopub.status.idle":"2025-10-19T10:20:54.062372Z","shell.execute_reply.started":"2025-10-19T10:19:44.487051Z","shell.execute_reply":"2025-10-19T10:20:54.061609Z"}},"outputs":[{"name":"stderr","text":"Processing /kaggle/input/neuro-icu/note/note/discharge.csv: 7it [00:33,  4.82s/it]\nProcessing /kaggle/input/neuro-icu/note/note/radiology.csv: 47it [00:27,  1.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"ðŸ§¾ Total Neuro ICU admissions with notes: 3780\nâœ… Saved clean + tagged Neuro ICU notes â†’ 'clean_neuro_notes_tagged.csv'\n\nðŸ“Š Average note length (chars): 2838\nðŸ§© Condition distribution (unique admissions):\n Hemorrhage         3147\nIschemic_Stroke    2287\nSeizure            1853\nBrain_Tumor        1019\nHydrocephalus      1002\ndtype: int64\n\nðŸ§¾ Total Neuro ICU admissions: 6884\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================\n# 8. Merge condition tags + Feature Scaling + Train/Test Split\n# ============================================================\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# ----------------------------------------\n# 1. Load base structured dataset\n# ----------------------------------------\nmodel_df = pd.read_csv(\"model_ready_neuro_6h.csv\")\n\n# ----------------------------------------\n# 2. Load condition tags (from notes)\n# ----------------------------------------\ncond_df = pd.read_csv(\"clean_neuro_notes_tagged.csv\")\n\n# Keep only required columns for merge\ncondition_cols = [\n    \"hadm_id\",\n    \"Ischemic_Stroke\", \"Hemorrhage\", \"Seizure\", \"Brain_Tumor\", \"Hydrocephalus\"\n]\n\ncond_df = cond_df[condition_cols].drop_duplicates(subset=\"hadm_id\")\n\n# Merge using hadm_id\nmodel_df = model_df.merge(cond_df, on=\"hadm_id\", how=\"left\")\n\n# Fill missing condition flags with 0 (patients without matching notes)\nfor c in condition_cols[1:]:\n    model_df[c] = model_df[c].fillna(0).astype(int)\n\nprint(f\"âœ… After merging notes: {model_df.shape}\")\nprint(f\"Columns now include: {list(model_df.columns)}\")\n\n# ----------------------------------------\n# 3. Select feature columns\n# ----------------------------------------\nfeature_cols = [\n    \"vital_mean\", \"vital_std\", \"vital_min\", \"vital_max\", \"vital_count\",\n    \"total_input_ml\", \"total_output_ml\", \"los\"\n]\nneuro_flags = [\n    \"Ischemic_Stroke\", \"Hemorrhage\", \"Seizure\", \"Brain_Tumor\", \"Hydrocephalus\"\n]\n\n# ----------------------------------------\n# 4. Define X and y\n# ----------------------------------------\nX = model_df[feature_cols + neuro_flags].copy()\ny = model_df[\"ventilation_within_12h\"].astype(int)\n\n# Replace inf/nan values\nX = X.replace([np.inf, -np.inf], np.nan)\nX = X.fillna(X.median())\n\n# ----------------------------------------\n# 5. Train-test split (stratified)\n# ----------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# ----------------------------------------\n# 6. Feature scaling (StandardScaler)\n# ----------------------------------------\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"\\nâœ… Data ready for modeling!\")\nprint(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\nprint(f\"Positive label proportion (train): {y_train.mean():.3f}\")\nprint(f\"Positive label proportion (test):  {y_test.mean():.3f}\")\n\n# ----------------------------------------\n# 7. Save Scaled Datasets\n# ----------------------------------------\ntrain_df = pd.DataFrame(X_train_scaled, columns=X.columns)\ntrain_df[\"label\"] = y_train.values\n\ntest_df = pd.DataFrame(X_test_scaled, columns=X.columns)\ntest_df[\"label\"] = y_test.values\n\ntrain_df.to_csv(\"train_neuro6h_scaled.csv\", index=False)\ntest_df.to_csv(\"test_neuro6h_scaled.csv\", index=False)\n\nprint(\"\\nðŸ’¾ Saved:\")\nprint(\"- train_neuro6h_scaled.csv\")\nprint(\"- test_neuro6h_scaled.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T10:22:36.557629Z","iopub.execute_input":"2025-10-19T10:22:36.558342Z","iopub.status.idle":"2025-10-19T10:22:36.791144Z","shell.execute_reply.started":"2025-10-19T10:22:36.558320Z","shell.execute_reply":"2025-10-19T10:22:36.790501Z"}},"outputs":[{"name":"stdout","text":"âœ… After merging notes: (7197, 20)\nColumns now include: ['subject_id', 'hadm_id', 'stay_id', 'first_careunit', 'intime', 'outtime', 'los', 'ventilation_within_12h', 'vital_mean', 'vital_std', 'vital_min', 'vital_max', 'vital_count', 'total_input_ml', 'total_output_ml', 'Ischemic_Stroke', 'Hemorrhage', 'Seizure', 'Brain_Tumor', 'Hydrocephalus']\n\nâœ… Data ready for modeling!\nTrain shape: (5757, 13), Test shape: (1440, 13)\nPositive label proportion (train): 0.138\nPositive label proportion (test):  0.138\n\nðŸ’¾ Saved:\n- train_neuro6h_scaled.csv\n- test_neuro6h_scaled.csv\n","output_type":"stream"}],"execution_count":14}]}